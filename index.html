<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FDM Additive Manufacturing Online Monitoring and Control</h1>
            <h2 class="subtitle is-3 publication-subtitle">XJTU Datasets and Publications</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">ZHANG Huaqing</a>,</span>
                <span class="author-block">
                  <a href="https://zhaozhibin.github.io/" target="_blank">Zhibin Zhao</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yu Bowen</a>
                    <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhang Xingwu</a>
                    <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">CHen Xuefeng</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">School of Mechanical Engineering, Xi’an Jiaotong University</span>
                    <span class="eql-cntrb"><small><br>Contact: zhang1qing2hua3@stu.xjtu.edu.cn (ZHANG Huaqing), zhaozhibin@stu.xjtu.edu.cn (Zhao Zhibin), yubowen@stu.xjtu.edu.cn (Yu Bowen)</small></span>
                  </div>


        </div>
      </div>
    </div>
  </div>
</section>

<!-- Website Introduction -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content has-text-justified">
          <div class="box">
            <p class="is-size-5">
              Welcome to the research showcase of <strong>Professor Zhaozhibin's group</strong> in <strong>fused deposition modeling (FDM) additive manufacturing monitoring</strong>. This webpage systematically presents <strong>three significant academic publications</strong> covering the complete technical roadmap from transfer learning and multi-view monitoring to multimodal large models. All datasets and source codes are publicly available, providing valuable research resources for both academia and industry. <strong>Click the tabs below to explore detailed content of each publication.</strong>
            </p>
            
            <div class="content">
              <h4 class="title is-5">The three publications are:</h4>
              <ol>
                <li><strong>"Mitigating Domain Shift in Online Process Monitoring for Material Extrusion Additive Manufacturing via Transfer Learning"</strong> (<a href="https://linkinghub.elsevier.com/retrieve/pii/S221486042400513X" target="_blank">Paper Link</a>) - <em>Additive Manufacturing</em>, 2024</li>
                <li><strong>"Multi-view deep information fusion framework for online quality monitoring and autonomous correction in material extrusion additive manufacturing"</strong> (<a href="https://www.tandfonline.com/doi/full/10.1080/17452759.2025.2500672" target="_blank">Paper Link</a>) - <em>Virtual and Physical Prototyping</em>, 2025</li>
                <li><strong>"Multimodal-Large-Model-Driven Online Monitoring for Fused-Deposition Additive Manufacturing"</strong> - <em>Chinese Journal of Mechanical Engineering</em>, 2025</li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Website Introduction -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video src="static/images/real_time_monitor.mp4" alt="AM Real Time Monitor" style="width: 100%; height: auto;" autoplay muted loop controls>
        Your browser does not support the video tag.
      </video>
      <h2 class="subtitle has-text-centered">
        Real-time monitor and feedback control of FDM additive manufacturing. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Publications Section with Tabs -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Our Research Publications</h2>
        
        <!-- Tabs Navigation -->
        <div class="tabs is-centered is-boxed is-large">
          <ul>
            <li class="is-active" data-tab="paper1">
              <a>
                <span class="icon is-small"><i class="fas fa-share-alt"></i></span>
                <span>Transfer Learning in AM</span>
              </a>
            </li>
            <li data-tab="paper2">
              <a>
                <span class="icon is-small"><i class="fas fa-eye"></i></span>
                <span>Multi-View Learning in AM</span>
              </a>
            </li>
            <li data-tab="paper3">
              <a>
                <span class="icon is-small"><i class="fas fa-brain"></i></span>
                <span>Multimodal Large Model</span>
              </a>
            </li>
          </ul>
        </div>
        
        <!-- Tab Content -->
        <div class="tab-content">
          <!-- Paper 1 Content -->
          <div id="paper1" class="tab-pane is-active">
            <div class="content has-text-justified">
              <h3 class="title is-4">Mitigating Domain Shift in Online Process Monitoring for Material Extrusion Additive Manufacturing via Transfer Learning</h3>
              <p class="subtitle is-6"><strong>Journal:</strong> Additive Manufacturing, 2024</p>
              
              <!-- Paper 1 Links -->
              <div class="has-text-centered mb-4">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://linkinghub.elsevier.com/retrieve/pii/S221486042400513X" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://www.repository.cam.ac.uk/items/6d77cd6d-8569-4bf4-9d5f-311ad2a49ac8" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset 1</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/zhanghuaqing/XJTU_AM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset 2</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://github.com/zhanghuaqing123/Transfer-Learning-Framework-in-AM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2024.transfer.learning" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Abstract</h4>
                <p>
                  This study addresses the commonly overlooked <strong>domain shift problem</strong> in online process monitoring for <strong>Material Extrusion Additive Manufacturing</strong>. We developed an integrated experimental platform with an in-situ <strong>evaluation and control system</strong>, which systematically reveals how <strong>environmental factors</strong> (e.g., lighting, vibration) and printing <strong>task variations</strong> (e.g., material color, part geometry, device differences) cause <strong>data distribution discrepancies</strong>. These discrepancies significantly degrade the generalization performance of deep learning models (e.g., accuracy drops from <strong>86% in the source domain to 44% in the target domain</strong>).
                </p>
                <p>
                  To tackle this challenge, we designed an <strong>adaptive transfer learning framework</strong>:
                </p>
                <ul>
                  <li><strong>Supervised fine-tuning</strong> when limited target-domain labels are available;</li>
                  <li><strong>Unsupervised feature alignment</strong> (e.g., adversarial domain adaptation via DANN, MMD-based distance alignment via TCA) when target labels are absent.</li>
                </ul>
                <p>
                  Experimental validation confirms that this framework <strong>enhances model robustness</strong> in real-world production environments. It enables <strong>real-time image feedback for online parameter correction</strong>, paving the way for transferring lab-developed monitoring technologies to industrial applications.
                </p>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Dataset Description</h4>
                <p>
                  This study utilizes the open-source dataset from the <strong>University of Cambridge</strong> <mcreference link="https://www.repository.cam.ac.uk/items/6d77cd6d-8569-4bf4-9d5f-311ad2a49ac8" index="0">0</mcreference>, which was originally developed for the research "<em>Generalisable 3D printing error detection and correction via multi-head neural networks</em>". The dataset contains <strong>1,272,273 labelled images</strong> of the extrusion 3D printing process, captured using a camera mounted next to the printer nozzle during material deposition for <strong>192 different printed parts</strong> covering a range of geometries, material colours, and lighting conditions <mcreference link="https://www.repository.cam.ac.uk/items/6d77cd6d-8569-4bf4-9d5f-311ad2a49ac8" index="0">0</mcreference>.
                </p>
                <p>
                  Each image in the dataset is comprehensively labelled with critical process parameters including <strong>flow rate, lateral speed, Z offset, hotend temperature, hotend target temperature, bed temperature, timestamp, and nozzle tip coordinates</strong> <mcreference link="https://www.repository.cam.ac.uk/items/6d77cd6d-8569-4bf4-9d5f-311ad2a49ac8" index="0">0</mcreference>. The data was collected through an automated pipeline from a fleet of <strong>8 extrusion printers</strong> with systematic sampling of different printing parameter combinations.
                </p>
                <p>
                  <strong>Domain Shift Challenge:</strong> A critical observation from this dataset is that <strong>significant domain shifts exist between almost all sub-datasets</strong> due to variations in printer configurations, environmental conditions, material properties, and geometric complexities. These domain discrepancies pose substantial challenges for model generalization across different printing scenarios, which forms the core motivation for our transfer learning framework.
                </p>
                <p>
                  <strong>Experimental Validation:</strong> In our study, we specifically selected and utilized <strong>datasets 109, 131, and 132</strong> from this comprehensive collection to validate our proposed adaptive transfer learning approach. These datasets represent diverse printing conditions and demonstrate varying degrees of domain shift, providing an ideal testbed for evaluating the effectiveness of our domain adaptation strategies.
                </p>
                <p>
                  <strong>Dataset 2 Description:</strong> In addition to the Cambridge dataset, we also utilize our self-collected dataset from <strong>Dataset 2</strong>. Specifically, the data file <strong>"20240125215540dataset_switch=长方体.zip"</strong> is directly related to the first paper's research. This dataset was collected using the same methodology as described for Dataset 1, but using our own equipment setup. The data collection process follows identical procedures and parameter configurations, ensuring consistency in data quality and labeling standards. Other datasets in the Dataset 2 link are primarily applicable to the third paper's research, where only the <strong>nozzle flow rate variable</strong> is systematically varied while maintaining other parameters constant.
                </p>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Method Overview</h4>
                <figure class="image">
                  <img src="static/images/AM_method_overview.jpg" alt="AM Method Overview" style="width: 100%; height: auto;">
                </figure>
                <p class="has-text-centered mt-3">
                  <em>Overview of the adaptive transfer learning framework for domain shift in additive manufacturing process monitoring.</em>
                </p>
              </div>
            </div>
          </div>
          
          <!-- Paper 2 Content -->
          <div id="paper2" class="tab-pane" style="display: none;">
            <div class="content has-text-justified">
              <h3 class="title is-4">Multi-view deep information fusion framework for online quality monitoring and autonomous correction in material extrusion additive manufacturing</h3>
              <p class="subtitle is-6"><strong>Journal:</strong> Virtual and Physical Prototyping, 2025</p>
              
              <!-- Paper 2 Links -->
              <div class="has-text-centered mb-4">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://www.tandfonline.com/doi/full/10.1080/17452759.2025.2500672" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://pan.baidu.com/s/1dvlS9zvqnFFaDOL6v8J4-A?pwd=r2x2" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset 1</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://pan.baidu.com/s/1nkGSpvOKb4Sx0QQKMqSLHA?pwd=i039" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset 2</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://github.com/ybw110/FDM.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2024.multiview.am" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Abstract</h4>
                <p>
                  This study targets the critical issue that <strong>traditional single-view online monitoring approaches</strong> for <strong>Fused Filament Fabrication (FFF) 3D printing</strong> suffer from low defect detection accuracy and limited correction capabilities when faced with complex structures, occlusions, and limited defect visibility.
                </p>
                <p>
                  <strong>Key Innovations:</strong>
                </p>
                <ul>
                  <li><strong>Multi-view monitoring system</strong> based on Ender-3 S1 platform with three high-resolution cameras at different angles</li>
                  <li><strong>Automated acquisition pipeline</strong> for systematic capture and annotation of synchronized multi-view images</li>
                  <li><strong>Large-scale dataset</strong> of 249,036 images with rigorous classification into five typical defect categories</li>
                  <li><strong>Multi-view deep information fusion algorithms</strong> achieving over 97% defect detection accuracy</li>
                  <li><strong>Real-time adaptive parameter correction</strong> for closed-loop optimization</li>
                </ul>
                <p>
                  <strong>Technical Achievements:</strong>
                </p>
                <ul>
                  <li>Systematic data collection under various parameter settings, geometric models, and dynamic natural lighting conditions</li>
                  <li>Each data sample paired with detailed process parameter records</li>
                  <li>Substantially enhanced intelligent quality monitoring capabilities</li>
                  <li>Comprehensive foundation for multi-view defect detection and feature fusion research</li>
                </ul>
                <p>
                  This dataset offers a <strong>systematic and comprehensive foundation</strong> for research and applications in multi-view defect detection, feature fusion, and intelligent process control, supporting both theoretical advancements and industrial practice in <strong>smart manufacturing</strong>.
                </p>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Dataset Description</h4>
                
                <h5 class="title is-6"><strong>2.1 Dataset Overview</strong></h5>
                <p>
                  This dataset is specifically designed for <strong>multi-view online quality monitoring and autonomous error correction</strong> research in <strong>Fused Filament Fabrication (FFF)</strong> additive manufacturing processes. Data collection is conducted around actual 3D printer platforms, focusing on two major application scenarios: <strong>defect detection and process control</strong>. The dataset systematically records imaging and parameter information during the printing process under different process parameters, different part geometries, and diverse natural lighting environments.
                </p>
                <p>
                  Its most distinctive feature lies in the <strong>synchronized provision of real-time image data from three typical viewpoints</strong> along with <strong>full-process automated defect labeling</strong>, focusing on common defect states such as over-extrusion and under-extrusion. The rich sample sources and highly accurate annotation information provide a solid foundation for algorithm development and performance evaluation in <strong>deep learning, multi-view information fusion, and intelligent manufacturing process monitoring</strong>.
                </p>
                
                <h5 class="title is-6"><strong>2.2 Data Collection and Experimental Platform</strong></h5>
                <p><strong>1. Hardware Platform:</strong></p>
                <ul>
                  <li><strong>Printer Model:</strong> Ender-3 S1 FFF 3D Printer</li>
                  <li><strong>Cameras:</strong> Three Logitech C270 cameras, resolution 1280×720, acquisition frequency 2.5Hz</li>
                  <li><strong>Control and Processing Unit:</strong> Raspberry Pi 4B (running Raspbian and OctoPrint), responsible for synchronized acquisition and G-code parameter adjustment</li>
                </ul>
                
                <p><strong>2. Camera Layout and Viewpoints:</strong></p>
                <ul>
                  <li><strong>45° viewpoint:</strong> Balances vertical and horizontal features, suitable for overall observation</li>
                  <li><strong>60° viewpoint:</strong> Moderately supplements information from different spatial levels</li>
                  <li><strong>90° viewpoint (side view):</strong> Focuses on capturing inter-layer bonding and vertical surface quality</li>
                </ul>
                <p>
                  Three cameras synchronously capture the printing state at the same moment, ensuring <strong>multi-view spatiotemporal consistency</strong> and effectively overcoming occlusion and blind spot problems.
                </p>
                
                <p><strong>3. Data Collection Process:</strong></p>
                <p>Two strategies are employed to generate diverse defect and normal samples:</p>
                <ul>
                  <li><strong>Typical parameter combinations:</strong> Systematically set flow rate and feed rate to low/normal/high multiple combinations, each group with rich test samples, achieving full coverage from severe under-extrusion to severe over-extrusion</li>
                  <li><strong>Automatic parameter perturbation:</strong> Real-time monitoring with random dynamic parameter adjustments during the process, making the printing process automatically experience different defect states, enhancing data richness and authenticity</li>
                </ul>
                <p>
                  Synchronized collection of three-viewpoint images and corresponding labels, with all photos accompanied by precise G-code parameter records.
                </p>
                
                <p><strong>4. Printing Objects:</strong></p>
                <p>
                  Includes <strong>5 types of typical geometric complexity</strong> printing parts (such as strips, cubes, boats, sculptures, etc.), covering both simple and complex structures.
                </p>
                
                <h5 class="title is-6"><strong>2.3 Data Content and Annotation Structure</strong></h5>
                <p>
                  Each data group contains <strong>three images</strong>, recording close-ups of the nozzle and deposition area from 45°, 60°, and 90° viewpoints respectively. All images have been center-cropped and scaled to ensure consistent feature regions and meet network input requirements. Supporting labels cover <strong>five major printing states</strong> — severe under-extrusion, mild under-extrusion, normal, mild over-extrusion, and severe over-extrusion. All data strictly correspond to actual printing flow rate and feed rate parameter combinations.
                </p>
                <p>
                  To enhance data diversity and generalization capability, parameter perturbations and lighting variations were intentionally introduced during the experimental process. Label information is uniformly stored in the <strong>labels.csv file</strong>, with complete annotations by group number, viewpoint, category, and parameters.
                </p>
                <p>
                  The entire dataset contains a total of <strong>83,012 groups</strong>, each group containing three-viewpoint images, totaling <strong>249,036 images</strong>. The dataset is divided into training, validation, and test sets in a <strong>7:1.5:1.5 ratio</strong>, ensuring sufficient generalization capability for models under multi-class imbalanced conditions.
                </p>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Multi-View System Architecture</h4>
                <figure class="image">
                  <img src="static/images/Multi-view.png" alt="Multi-View System Architecture" style="width: 100%; height: auto;">
                </figure>
                <p class="has-text-centered mt-3">
                  <em>Multi-view online monitoring system with three-camera setup for comprehensive defect detection in FFF 3D printing.</em>
                </p>
              </div>
            </div>
          </div>
          
          <!-- Paper 3 Content -->
          <div id="paper3" class="tab-pane" style="display: none;">
            <div class="content has-text-justified">
              <h3 class="title is-4">Multimodal-Large-Model-Driven Online Monitoring for Fused-Deposition Additive Manufacturing</h3>
              <p class="subtitle is-6"><strong>Journal:</strong> Chinese Journal of Mechanical Engineering, 2024</p>
              
              <!-- Paper 3 Links -->
              <div class="has-text-centered mb-4">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/zhanghuaqing/XJTU_AM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://github.com/zhanghuaqing123/Multimodal-Large-Model-AM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>
                  
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2024.multimodal.large.model" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Abstract</h4>
                <p>
                  We introduce the first <strong>multimodal large-model framework</strong> for <strong>fused-deposition modeling (FDM)</strong> that closes the loop between sensing, diagnosis, and control.
                </p>
                <p>
                  <strong>Key breakthroughs:</strong>
                </p>
                <ul>
                  <li><strong>Multimodal perception:</strong> 120k in-process images (PLA/ABS, cubes/wrenches/embossed parts) coupled with hierarchical text labels create aligned vision–language samples</li>
                  <li><strong>Zero-/few-shot generalization:</strong> CLIP-based contrastive learning links visual cues to process semantics. Zero-shot reaches 46.16% mean accuracy (+9.09% vs. ResNet18); 15-shot tuning hits 91.10% (+29.51% vs. ViT-B/16)</li>
                  <li><strong>Real-time closed-loop control:</strong> A sliding-window feedback policy corrects anomalies (140% over-extrusion → 105%) within 10s, cutting porosity by 80% and boosting inter-layer strength by 35%</li>
                </ul>
                <p>
                  <strong>Impact:</strong>
                </p>
                <ul>
                  <li>The system shatters the generalization limits of single-modal models</li>
                  <li>Adapts instantly to new materials and geometries</li>
                  <li>Offers industry-grade FDM monitoring with minimal labeling</li>
                </ul>
              </div>
              
              <div class="box">
                <h4 class="title is-5">Multimodal Framework Architecture</h4>
                <div class="notification is-info is-light">
                  <p class="has-text-centered">
                    <i class="fas fa-info-circle"></i>
                    Multimodal large model framework diagram will be available soon.
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End publications section -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{ZHANG2024104467,
title = {Mitigating domain shift in online process monitoring for material extrusion additive manufacturing via transfer learning},
journal = {Additive Manufacturing},
volume = {94},
pages = {104467},
year = {2024},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2024.104467},
url = {https://www.sciencedirect.com/science/article/pii/S221486042400513X},
author = {Huaqing Zhang and Zhibin Zhao and Chenxi Wang and Xingwu Zhang and Xuefeng Chen},
keywords = {Online process evaluation, Domain shift, Monitoring system, Transfer learning framework},
abstract = {The additive manufacturing (AM) method has experienced rapid growth in recent decades. However, its application in end-use products is constrained by printing defects. Therefore, online process evaluation and optimization are crucial for producing parts that meet quality standards. Data-driven methods have been extensively employed for in-situ process evaluation and control in various AM processes to establish correlations among process parameters, process information, and part quality. However, a critical weakness of these methods is their poor generalization, primarily due to two factors: the diverse nature of printing itself and the significant influence of environmental factors. This study proposes a framework for in-situ process evaluation and control of AM using transfer learning to address the domain shift problems. It highlights a significant domain shift issue in AM monitoring, especially when using deep learning models. A comprehensive monitoring system for material extrusion additive manufacturing is constructed to collect data from different domains, emphasizing the notable differences in data characteristics across these domains. Lastly, transfer learning methods are tested to tackle the domain shift issue in AM, enhancing the robustness and reliability of the monitoring system.}
}

@article{Yu31122025,
author = {Bowen Yu and Huaqing Zhang and Jiafeng Tang and Zhibin Zhao and Xingwu Zhng and Weifeng He and Xuefeng Chen},
title = {Multi-view deep information fusion framework for online quality monitoring and autonomous correction in material extrusion additive manufacturing},
journal = {Virtual and Physical Prototyping},
volume = {20},
number = {1},
pages = {e2500672},
year = {2025},
publisher = {Taylor \& Francis},
doi = {10.1080/17452759.2025.2500672},
URL = {https://doi.org/10.1080/17452759.2025.2500672},
eprint = {https://doi.org/10.1080/17452759.2025.2500672}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
